{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MUSHROOM\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#[8124 rows x 23 columns]\n",
    "#Random numbers work by starting with a number (the seed), then the resulting no is the seed.\n",
    "seed = 1\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Loading the dataset\n",
    "data = pd.read_csv('mushrooms.csv', delimiter=',', header=None)\n",
    "data.head()\n",
    "\n",
    "initial = data.shape\n",
    "print initial\n",
    "#UCI.edu says: Missing Attribute Values: 2480 of them (denoted by \"?\"), all for attribute #11.\n",
    "#so we dissmiss them\n",
    "data = data[data[11] != '?'] #if stalk root entry is unkonown will not be memorized in data\n",
    "X = data.loc[:, data.columns != [0]] #label-based, we memorize the features in X, apart from labels\n",
    "X.head()\n",
    "print X.shape\n",
    "y = data[0].to_frame() #table like data, as X is structured already\n",
    "y.head()\n",
    "print y.shape\n",
    "final = data.shape\n",
    "print initial[0] - final[0]\n",
    "#our data is categorical so we'll need to encode data to train clf. get dummies from pandas will do\n",
    "X_numerical= pd.get_dummies(X)\n",
    "X_numerical.head()\n",
    "y_numerical = pd.get_dummies(y)\n",
    "#print y_numerical\n",
    "y = y_numerical\n",
    "X = X_numerical\n",
    "print y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrain, xTest, yTrain, yTest = train_test_split(X, y,  test_size=.3)\n",
    "#print yTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stochastic Gradient Descent (SGD) computes derivatives (of the squared error cost wrt weights and bias for example)\n",
    "#in case of deep MLPs that need to be trained on large amount of data\n",
    "my_clf = MLPClassifier(hidden_layer_sizes=(10),solver='sgd',learning_rate_init=0.01,max_iter=500)\n",
    "my_clf.fit(xTrain, yTrain)\n",
    "predictions = my_clf.predict(xTest)\n",
    "print predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#0 will mean edible 1 will mean not\n",
    "yTest = np.array(yTest)\n",
    "print yTest\n",
    "for i in range(len(yTest)):\n",
    "    if yTest[i]:\n",
    "        print yTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing to see how well it did\n",
    "print accuracy_score(yTest, predictions)\n",
    "print('How predictions should look', yTest)\n",
    "print('\\nOutputs are ', predictions)\n",
    "print('errors are ', abs(yTest-predictions))\n",
    "print('\\nNo of errors is ', sum(abs(yTest-predictions)))\n",
    "\n",
    "print('\\nConfusion matrix is\\n',confusion_matrix(yTest, predictions))\n",
    "print ('\\nPerformance is ',my_clf.score(xTest,yTest))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
